# Sigmoid (시그모이드 함수)

## 시그모이드 함수 (Sigmoid Function)

**1. 정의:**

시그모이드 함수는 입력 값의 범위를 (보통 0과 1 사이) 압축하여 출력 값을 반환하는 비선형 활성화 함수입니다. 주로 이진 분류 문제에서 사용되는 확률 값을 출력하는 데 활용됩니다.

**2. 핵심 개념:**

*   **비선형성:** 시그모이드 함수는 선형 함수가 아닌 비선형 함수로, 복잡한 패턴을 학습하는 데 필수적입니다.
*   **확률 출력:** 0과 1 사이의 값을 출력하여, 특정 클래스에 속할 확률을 나타냅니다.
*   **S자 곡선:** 그래프로 나타내면 S자 모양의 곡선을 가지며, 이 곡선 형태가 중요한 특징입니다.
*   **활성화 함수:** 신경망의 각 뉴런에서 입력 신호를 처리하는 역할을 합니다.
*   **경사 하강법:** 시그모이드 함수의 미분 값을 이용하여 신경망의 가중치를 업데이트하는 데 사용됩니다.

**3. 작동 방식:**

시그모이드 함수는 다음과 같은 수식으로 정의됩니다:

σ(x) = 1 / (1 + e^(-x))

여기서:

*   σ(x)는 시그모이드 함수의 출력 값입니다.
*   x는 입력 값입니다.
*   e는 자연상수 (약 2.71828) 입니다.

입력 값 x가 커질수록 (음수 무한대 방향으로) 시그모이드 함수의 출력 값은 1에 가까워지고, 입력 값 x가 작아질수록 (양수 무한대 방향으로) 출력 값은 0에 가까워집니다. 입력 값이 0일 때는 출력 값은 0.5가 됩니다.

**4. 응용 분야:**

*   **이진 분류 문제:** 이메일 스팸 분류, 이미지 인식 (고양이 vs 개), 의료 진단 등에서 클래스에 속할 확률을 예측하는 데 사용됩니다.
*   **신경망:**  신경망의 출력층에서 주로 사용되어, 각 클래스에 대한 확률 값을 출력합니다.
*   **생성적 적대 신경망 (GAN):**  GAN의 생성자 네트워크에서 이진 분류를 수행하는 데 사용됩니다.

**5. 관련 용어:**

*   **ReLU (Rectified Linear Unit):** 또 다른 활성화 함수로, 시그모이드 함수보다 더 효율적으로 학습될 수 있습니다.
*   **tanh (Hyperbolic Tangent):** 시그모이드 함수와 유사하지만, 출력 범위가 -1과 1 사이입니다.
*   **층간 정규화 (Batch Normalization):** 신경망 학습 속도를 높이는 기술로, 시그모이드 함수의 출력을 안정화하는 데 사용될 수 있습니다.
*   **가중치 초기화 (Weight Initialization):** 신경망 학습의 성능에 영향을 미치는 중요한 요소이며, 시그모이드 함수의 학습에 영향을 줄 수 있습니다.
*   **미분 (Derivative):** 시그모이드 함수의 학습에 사용되는 핵심 개념으로,  가중치 업데이트에 사용됩니다.