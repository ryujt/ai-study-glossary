# Multi‑Head Attention (다중 헤드 어텐션)

**1. 정의:**

다중 헤드 어텐션(Multi-Head Attention)은 Transformer 모델의 핵심 구성 요소로, 입력 시퀀스 내의 각 단어 간의 관계를 여러 개의 독립적인 ‘헤드’를 통해 동시에 학습하여 더 풍부하고 다양한 정보를 추출하는 기법입니다.

**2. 핵심 개념:**

*   **어텐션(Attention):** 입력 시퀀스의 각 부분(단어)이 다른 부분과 얼마나 관련이 있는지 측정하는 메커니즘입니다.
*   **선형 변환(Linear Transformation):** 각 헤드에서 어텐션 가중치를 계산하기 전에 입력 데이터를 여러 개의 다른 공간으로 투영하는 과정입니다.
*   **가중 합(Weighted Sum):** 어텐션 가중치를 사용하여 입력 데이터의 각 부분을 가중 평균하여 더 중요한 정보를 강조합니다.
*   **병렬 처리(Parallel Processing):** 여러 헤드가 동시에 작동하여 계산 효율성을 높입니다.
*   **인코딩(Encoding):** 입력 시퀀스의 정보를 고차원 공간으로 표현하는 과정입니다.

**3. 작동 방식:**

1.  **입력 임베딩:** 먼저 입력 시퀀스의 각 단어를 벡터 형태로 변환합니다.
2.  **선형 투영:** 각 헤드는 입력 벡터를 다른 수의 선형 변환을 통해 3개의 새로운 벡터 공간(Query, Key, Value)으로 투영합니다.
3.  **어텐션 가중치 계산:** 각 헤드에서 Query 벡터와 Key 벡터 간의 유사도를 계산하여 어텐션 가중치를 얻습니다. 일반적으로 Dot-Product attention을 사용합니다.
4.  **가중 합:** 어텐션 가중치를 사용하여 Value 벡터를 가중 평균합니다.
5.  **결합 및 선형 변환:** 모든 헤드의 출력을 연결하고, 최종적으로 선형 변환을 통해 단일 출력 벡터를 생성합니다.

**4. 응용 분야:**

*   **자연어 처리(NLP):** 기계 번역, 텍스트 요약, 질의 응답 등 다양한 NLP 작업에서 뛰어난 성능을 보입니다.
*   **컴퓨터 비전(Computer Vision):** 이미지 캡셔닝, 객체 탐지 등 시각적인 작업에도 적용되고 있습니다.
*   **시계열 데이터 분석(Time Series Analysis):** 주가 예측, 센서 데이터 분석 등 시계열 데이터 분석에도 활용됩니다.

**5. 관련 용어:**

*   **셀프 어텐션(Self-Attention):** 동일한 시퀀스 내의 단어 간 관계를 학습하는 어텐션의 한 종류입니다.
*   **레이어 정규화(Layer Normalization):** 각 레이어의 출력을 정규화하여 학습 안정성을 높이는 기술입니다.
*   **Residual Connection(Skip Connection):** 입력과 출력을 연결하여 기울기 소실 문제를 완화하는 기법입니다.
*   **Positional Encoding:** 시퀀스의 단어 위치 정보를 모델에 제공하는 방법입니다.
*   **Transformer:** 다중 헤드 어텐션 기반의 신경망 아키텍처입니다.