# Self‑Attention (셀프 어텐션)

## 셀프 어텐션 (Self-Attention) 설명

**1. 정의:**

셀프 어텐션은 주어진 입력 시퀀스 내의 각 요소가 다른 모든 요소와 얼마나 관련이 있는지 학습하는 메커니즘입니다.  쉽게 말해, 문장 내 단어들 간의 관계를 파악하는 방법입니다.

**2. 핵심 개념:**

*   **쿼리(Query):** 현재 요소의 "질문" 역할을 합니다.
*   **키(Key):** 다른 요소들의 "답변"을 나타냅니다.
*   **값(Value):** 키에 해당하는 실제 정보입니다.
*   **어텐션 가중치(Attention Weight):** 쿼리와 각 키의 유사성을 기반으로 계산되어, 각 키에 얼마나 집중할지를 결정합니다.
*   **콘텍스트 벡터(Context Vector):** 어텐션 가중치를 사용하여 계산된 값들의 가중 합으로 생성되어, 입력 시퀀스의 전체적인 정보를 담고 있습니다.

**3. 작동 방식:**

1.  **임베딩:** 입력 시퀀스의 각 요소(예: 단어)를 벡터 형태로 변환합니다.
2.  **쿼리, 키, 값 생성:** 각 입력 벡터로부터 쿼리, 키, 값 벡터를 생성합니다.
3.  **어텐션 가중치 계산:** 쿼리와 각 키 벡터 간의 유사성을 계산합니다. 일반적으로 내적(dot product)을 사용하며, 그 결과값을 스케일링하여 안정성을 확보합니다.  활성화 함수(예: 소프트맥스)를 통해 가중치를 0과 1 사이 값으로 변환합니다.
4.  **가중 합 계산:** 어텐션 가중치를 사용하여 각 값 벡터를 가중 합합니다.  이 가중 합이 콘텍스트 벡터를 형성합니다.
5.  **콘텍스트 벡터 활용:** 콘텍스트 벡터를 다음 단계의 계산에 활용합니다 (예: 문장 분류, 기계 번역).

**4. 응용 분야:**

*   **자연어 처리(NLP):** 기계 번역, 텍스트 요약, 질의 응답, 감성 분석 등 다양한 NLP 작업에서 뛰어난 성능을 보입니다.
*   **이미지 처리:** 이미지 내 객체 간 관계 파악, 이미지 캡셔닝 등에 활용될 수 있습니다.
*   **시계열 분석:**  시계열 데이터 내의 장기적인 의존성을 파악하는 데 사용될 수 있습니다.

**5. 관련 용어:**

*   **트랜스포머(Transformer):** 셀프 어텐션을 기반으로 구축된 딥러닝 아키텍처입니다.
*   **인코더-디코더(Encoder-Decoder):**  트랜스포머 아키텍처의 구성 요소 중 하나로, 입력 시퀀스를 이해하고 목표 시퀀스를 생성하는 데 사용됩니다.
*   **신경망(Neural Network):** 셀프 어텐션이 사용되는 기본적인 딥러닝 모델입니다.
*   **벡터 임베딩(Vector Embedding):**  단어, 문장 등을 숫자로 표현하는 방식입니다.
*   **어텐션 메커니즘(Attention Mechanism):** 셀프 어텐션의 기반이 되는 핵심 개념입니다.