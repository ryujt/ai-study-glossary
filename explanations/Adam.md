# Adam (Adam 옵티마이저)

**1. 정의:**

Adam (Adam 옵티마이저)은 딥러닝 모델 학습에 사용되는 최적화 알고리즘입니다. 주로 확률적 경사 하강법(Stochastic Gradient Descent, SGD)의 변형으로, 신경망의 가중치를 조정하여 손실 함수를 최소화하는 데 사용됩니다.

**2. 핵심 개념:**

*   **경사 하강법:** 손실 함수의 기울기를 이용하여 가중치를 업데이트하는 방법입니다.
*   **확률적 경사 하강법 (SGD):** 전체 데이터셋 대신 일부 데이터(미니 배치)를 사용하여 경사를 계산하고 가중치를 업데이트합니다.
*   **모멘텀:** 과거 경사 정보를 활용하여 업데이트 방향을 조정하고, 지역 최소점에 갇히는 것을 방지합니다.
*   **RMSprop (Root Mean Square Propagation):** 각 가중치의 기울기의 크기에 따라 학습률을 조정하여 학습 속도를 조절합니다.

**3. 작동 방식:**

1.  **경사 계산:** 미니 배치의 데이터에 대해 손실 함수의 기울기를 계산합니다.
2.  **가중치 업데이트:** 계산된 기울기를 사용하여 각 가중치를 업데이트합니다. 이 때 모멘텀과 RMSprop의 개념을 활용하여 업데이트 방향과 학습률을 조정합니다.
3.  **방정식:** 일반적으로 다음 방정식으로 표현됩니다:  `w = w - learning_rate * gradient` (w는 가중치, learning_rate는 학습률, gradient는 기울기)

**4. 응용 분야:**

*   **이미지 분류:** 이미지 인식 모델 학습에 널리 사용됩니다.
*   **자연어 처리:** 번역, 텍스트 생성, 챗봇 등 다양한 자연어 처리 모델 학습에 활용됩니다.
*   **생성 모델:** GAN (Generative Adversarial Networks)과 같은 생성 모델 학습에 사용됩니다.
*   **컴퓨터 비전:** 객체 탐지, 이미지 분할 등 다양한 컴퓨터 비전 문제 해결에 적용됩니다.

**5. 관련 용어:**

*   **손실 함수 (Loss Function):** 모델의 예측값과 실제값의 차이를 나타내는 함수입니다.
*   **경사 하강법 (Gradient Descent):** 손실 함수를 최소화하기 위해 가중치를 반복적으로 업데이트하는 알고리즘입니다.
*   **미니 배치 (Mini-Batch):** 전체 데이터셋에서 일부 데이터를 선택하여 경사를 계산하는 데 사용되는 데이터 묶음입니다.
*   **학습률 (Learning Rate):** 가중치 업데이트의 크기를 결정하는 하이퍼파라미터입니다.
*   **방사형 소거 (Momentum):** 과거 경사 정보를 활용하여 업데이트 방향을 조정하여 학습 속도를 향상시키는 기술입니다.