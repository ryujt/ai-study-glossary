# Activation Function (활성화 함수)

**Activation Function (활성화 함수)**

1.  **정의:** Activation function은 인공신경망에서 각 뉴런의 출력을 결정하는 함수입니다. 입력 신호의 조합을 받아 출력 신호를 생성하여 신경망이 비선형적인 관계를 학습할 수 있도록 합니다.

2.  **핵심 개념:**
    *   비선형성: 신경망의 복잡한 패턴을 학습하기 위해 필수적입니다.
    *   출력 제한: 뉴런의 출력을 특정 범위로 제한하여 안정적인 학습을 돕습니다.
    *   학습률: 활성화 함수의 형태는 학습률에 영향을 미칩니다.
    *   신호 증폭/감쇠: 특정 입력 신호를 증폭하거나 감쇠시켜 중요한 정보에 집중할 수 있도록 합니다.

3.  **작동 방식:**
    *   신경망의 각 뉴런은 입력 신호를 받습니다.
    *   활성화 함수는 이 입력 신호를 받아 출력 신호를 생성합니다.
    *   예를 들어, ReLU 함수는 음수 값을 0으로 만들고 양수 값은 그대로 통과시킵니다. Sigmoid 함수는 출력을 0과 1 사이의 값으로 제한합니다.

4.  **응용 분야:**
    *   심층 신경망 (Deep Neural Networks): 복잡한 패턴 학습에 핵심적인 역할을 합니다.
    *   이미지 인식 (Image Recognition): 이미지 데이터의 특징을 추출하고 분류하는 데 사용됩니다.
    *   자연어 처리 (Natural Language Processing): 텍스트 데이터의 의미를 이해하고 생성하는 데 사용됩니다.

5.  **관련 용어:**
    *   가중치 (Weight): 입력 신호에 곱해지는 값.
    *   편향 (Bias): 활성화 함수 입력에 더해지는 상수 값.
    *   경사하강법 (Gradient Descent): 신경망의 가중치를 최적화하는 알고리즘.
    *   층 (Layer): 신경망의 구성 요소 중 하나.
