# Softmax (소프트맥스 함수)

## Softmax (소프트맥스 함수)

**1. 정의:**
소프트맥스 함수는 입력 값의 합이 1이 되는 확률 분포를 출력하는 데 사용되는 함수입니다. 각 출력 값은 해당 클래스에 속할 확률을 나타냅니다.

**2. 핵심 개념:**

*   **확률 분포:** 입력 값에 대한 가능성을 나타내는 분포를 의미합니다.
*   **클래스 (Class):**  분류 문제에서 가능한 결과의 각 종류를 의미합니다. (예: 고양이, 개, 새)
*   **활성화 함수:** 신경망의 출력 값을 조정하는 데 사용되는 함수입니다. 소프트맥스는 분류 문제에서 널리 사용되는 활성화 함수입니다.
*   **로그 확률 (Logit):** 소프트맥스 함수에 입력되는 값으로, 활성화 함수 출력 값의 로그 값을 의미합니다.
*   **미분 가능성:** 소프트맥스 함수는 미분 가능하므로, 경사 하강법과 같은 최적화 알고리즘에서 활용될 수 있습니다.

**3. 작동 방식:**

1.  **입력 값:** 소프트맥스 함수는 일반적으로 신경망의 마지막 레이어에서 출력된 로짓 (Logit) 값을 입력으로 받습니다.
2.  **지수 함수 (Exponential Function):** 각 로짓 값에 지수 함수를 적용합니다. 이는 각 로짓 값을 큰 양수로 변환합니다.
3.  **정규화 (Normalization):**  지수 함수를 통해 얻은 값들을 모두 더하면 1이 됩니다. 이 합계를 이용하여 각 값에 대해 정규화하여 확률 값을 얻습니다.  수식은 다음과 같습니다:

    σ(zᵢ) = exp(zᵢ) / Σ exp(zⱼ)  (여기서 j는 모든 클래스에 대한 인덱스)

**4. 응용 분야:**

*   **이미지 분류:**  CNN (Convolutional Neural Network) 모델의 마지막 레이어에서 이미지 분류를 위해 사용됩니다.
*   **자연어 처리 (NLP):** 텍스트 분류, 감성 분석, 기계 번역 등 다양한 NLP 작업에서 사용됩니다.
*   **음성 인식:**  음성 데이터를 텍스트로 변환하는 데 사용됩니다.
*   **추천 시스템:** 사용자에게 적합한 항목을 추천하는 데 사용됩니다.

**5. 관련 용어:**

*   **Cross-Entropy Loss:** 소프트맥스와 함께 사용되는 손실 함수입니다.
*   **One-Hot Encoding:**  다중 클래스 분류 문제에서 레이블을 표현하는 방식입니다.
*   **Sigmoid Function:**  출력 값을 0과 1 사이의 값으로 변환하는 활성화 함수입니다.
*   **ReLU (Rectified Linear Unit):**  신경망의 은닉 레이어에 자주 사용되는 활성화 함수입니다.
*   **Gradient Descent:** 경사 하강법은 손실 함수를 최소화하기 위해 사용되는 최적화 알고리즘입니다.