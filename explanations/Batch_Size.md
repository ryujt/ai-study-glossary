# Batch Size (배치 크기)

**1. 정의:**

배치 크기(Batch Size)는 딥러닝 모델 학습 시 한 번에 처리하는 데이터 샘플의 수를 의미합니다.

**2. 핵심 개념:**

*   **학습 속도:** 배치 크기는 학습 속도에 직접적인 영향을 미칩니다. 배치 크기가 클수록 학습 속도가 빨라질 수 있지만, 메모리 사용량도 증가합니다.
*   **메모리 제약:** GPU 메모리 용량에 따라 배치 크기가 제한될 수 있습니다.
*   **그래디언트 안정성:** 큰 배치 크기는 일반적으로 더 안정적인 그래디언트를 제공하여 학습을 더 안정화합니다.
*   **최적화 성능:** 적절한 배치 크기를 찾는 것은 모델의 최적화 성능에 매우 중요합니다.
*   **학습 시간:** 배치 크기가 클수록 전체 학습 시간은 길어질 수 있습니다.

**3. 작동 방식:**

*   데이터셋을 작은 배치로 나눕니다. 예를 들어, 1000개의 샘플이 있는 데이터셋을 배치 크기가 100인 경우 10개의 배치가 생성됩니다.
*   각 배치를 사용하여 모델을 학습합니다. 모델은 각 배치의 샘플에 대해 예측을 수행하고, 예측 오차를 기반으로 가중치를 업데이트합니다.
*   이 과정을 데이터셋 전체를 처리할 때까지 반복합니다.

**4. 응용 분야:**

*   **이미지 인식:** 이미지 데이터셋의 경우, 배치 크기는 이미지 크기와 GPU 메모리 용량에 따라 조정됩니다.
*   **자연어 처리:** 텍스트 데이터셋의 경우, 문장의 길이와 토큰 수에 따라 배치 크기가 결정됩니다.
*   **강화 학습:** 강화 학습 환경에서, 에피소드의 특정 부분을 배치로 사용하여 정책을 업데이트합니다.

**5. 관련 용어:**

*   **역전파(Backpropagation):** 모델 가중치를 업데이트하기 위한 핵심 알고리즘입니다.
*   **거치(Epoch):** 전체 데이터셋을 한 번 학습하는 것을 의미합니다.
*   **최적화 알고리즘(Optimization Algorithm):** 경사 하강법(Gradient Descent) 등 가중치를 업데이트하는 알고리즘입니다.
*   **그래디언트(Gradient):** 손실 함수를 가장 빠르게 감소시키는 방향을 나타냅니다.
*   **손실 함수(Loss Function):** 모델 예측과 실제 값 사이의 차이를 측정하는 함수입니다.
