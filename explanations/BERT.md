# BERT (양방향 인코더 표현)

## BERT (양방향 인코더 표현) 설명

**1. 정의:**

BERT는 Google에서 개발한 사전 훈련된 언어 모델입니다. Transformer 아키텍처를 기반으로 하며, 텍스트의 문맥을 이해하고 다양한 자연어 처리 작업을 수행할 수 있도록 설계되었습니다. "양방향"이라는 부분은 텍스트의 앞뒤 문맥 모두를 고려하여 단어의 의미를 파악한다는 것을 의미합니다.

**2. 핵심 개념:**

*   **Transformer 아키텍처:** BERT는 Self-Attention 메커니즘을 사용하는 Transformer 아키텍처를 기반으로 합니다. 이는 텍스트 내 단어 간의 관계를 효과적으로 파악하는 데 중요한 역할을 합니다.
*   **사전 훈련 (Pre-training):** 대량의 텍스트 데이터로 미리 훈련되어, 특정 작업에 맞게 미세 조정(Fine-tuning)할 때 성능이 훨씬 좋습니다.
*   **Masked Language Modeling (MLM):** BERT는 텍스트 내 일부 단어를 숨기고, 숨겨진 단어를 예측하도록 훈련됩니다. 이를 통해 텍스트 전체의 문맥을 이해하는 능력을 키웁니다.
*   **Next Sentence Prediction (NSP):** BERT는 두 문장이 이어지는 관계를 예측하는 훈련도 받습니다. 이는 문장 간의 관계를 파악하는 데 도움이 됩니다.
*   **Embedding:** 단어를 벡터 형태로 표현하여, 단어 간의 유사성을 계산하고 모델이 텍스트를 이해할 수 있도록 합니다.

**3. 작동 방식:**

BERT는 크게 두 단계로 작동합니다.

1.  **사전 훈련:** 대규모 텍스트 코퍼스(Wikipedia, BookCorpus 등)를 사용하여 MLM 및 NSP 작업을 통해 언어의 일반적인 패턴과 구조를 학습합니다.
2.  **미세 조정 (Fine-tuning):** 특정 자연어 처리 작업(예: 감성 분석, 질의 응답, 개체명 인식 등)에 맞게 BERT를 조정합니다. 이때, 사전 훈련된 모델의 지식을 활용하여 적은 데이터로도 높은 성능을 얻을 수 있습니다.

**4. 응용 분야:**

*   **감성 분석:** 텍스트의 긍정/부정 감성을 분석합니다.
*   **질의 응답:** 질문에 대한 답변을 텍스트에서 찾거나 생성합니다.
*   **개체명 인식:** 텍스트 내에서 사람, 장소, 조직 등과 같은 개체명을 식별합니다.
*   **기계 번역:** 한 언어에서 다른 언어로 텍스트를 번역합니다.
*   **텍스트 요약:** 긴 텍스트를 짧게 요약합니다.

**5. 관련 용어:**

*   **Word Embedding (단어 임베딩):** 단어를 벡터 형태로 표현하는 기술.
*   **Attention 메커니즘:** 모델이 입력 텍스트의 중요한 부분에 집중할 수 있도록 합니다.
*   **Sequence-to-Sequence 모델:** 입력 시퀀스를 다른 시퀀스로 변환하는 모델 (예: 기계 번역).
*   **Convolutional Neural Network (CNN):** 텍스트에서 특징을 추출하는 데 사용되는 딥러닝 모델.
*   **Recurrent Neural Network (RNN):** 순차적인 데이터를 처리하는 데 사용되는 딥러닝 모델 (BERT의 이전 버전에서 사용).
