# ReLU (Rectified Linear Unit, 렐루)

## ReLU (Rectified Linear Unit, 렐루) 설명

**1. 정의:**

ReLU는 활성화 함수 중 하나로, 신경망에서 0보다 작은 값은 0으로, 0보다 큰 값은 그대로 통과시키는 방식으로 뉴런의 출력을 결정합니다.

**2. 핵심 개념:**

*   **비선형성:** ReLU는 신경망에 비선형성을 추가하여 복잡한 패턴을 학습할 수 있도록 합니다.
*   **Vanishing Gradient 문제 완화:** 과거 활성화 함수 (Sigmoid, Tanh)의 Vanishing Gradient 문제를 어느 정도 완화합니다.
*   **계산 효율성:** 0 또는 양수 값만 계산하므로 Sigmoid나 Tanh보다 훨씬 빠릅니다.
*   **활성화 함수:** 신경망의 각 레이어에서 뉴런의 출력 값을 결정하는 역할을 합니다.
*   **경사하강법:** 신경망 학습 시, 손실 함수의 기울기를 이용하여 네트워크의 가중치를 업데이트하는 방법입니다.

**3. 작동 방식:**

*   입력 값(x)가 0보다 크면 x 그대로 출력됩니다.
*   입력 값(x)가 0보다 작으면 0을 출력합니다.
*   수식으로 표현하면 다음과 같습니다:  f(x) = max(0, x)

**4. 응용 분야:**

*   **딥러닝:** 이미지 인식, 자연어 처리, 음성 인식 등 다양한 딥러닝 모델의 Backbone으로 사용됩니다.
*   **CNN (Convolutional Neural Network):** 특히 Convolutional Neural Network에서 널리 사용되어 효율적인 학습을 가능하게 합니다.
*   **Recurrent Neural Network (RNN):** 일부 RNN 모델에서도 활용됩니다.

**5. 관련 용어:**

*   **Sigmoid 함수:**  출력 범위를 0~1 사이로 제한하는 활성화 함수입니다.
*   **Tanh 함수:**  Sigmoid 함수와 유사하지만, 출력 범위를 -1 ~ 1 사이로 제한합니다.
*   **손실 함수 (Loss Function):**  모델의 예측값과 실제값의 차이를 계산하는 함수입니다.
*   **경사하강법 (Gradient Descent):** 손실 함수를 최소화하기 위해 네트워크의 가중치를 업데이트하는 최적화 알고리즘입니다.
*   **Backpropagation (역전파):**  손실 함수의 기울기를 계산하여 네트워크의 가중치를 업데이트하는 과정입니다.