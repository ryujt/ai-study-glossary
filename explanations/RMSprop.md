# RMSprop (RMSprop 옵티마이저)

## RMSprop (RMSprop 옵티마이저)

**1. 정의:**

RMSprop (Root Mean Square Propagation)은 머신러닝, 특히 딥러닝 모델 학습에서 사용되는 적응적 학습률 옵티마이저입니다. 모델의 손실 함수를 최소화하기 위해 각 매개변수(파라미터)에 대해 학습률을 자동으로 조정합니다.

**2. 핵심 개념:**

*   **적응적 학습률 (Adaptive Learning Rate):** 각 매개변수에 대해 고정된 학습률 대신, 학습 진행 상황에 따라 학습률을 조절하는 방식입니다.
*   **분산 스무딩 (Decay):** 과거 기울기 값을 지수적으로 감쇠시켜, 최근 기울기 값에 더 큰 영향을 주도록 합니다.
*   **분산 스무딩 계수 (Decay Rate):** 과거 기울기 값을 감쇠시키는 속도를 결정하는 하이퍼파라미터입니다.
*   **분모 (Denominator):** 일반적으로, 분산 스무딩을 통해 계산된 제곱 기울기의 누적합을 사용합니다.
*   **Gradient Clipping:** 기울기가 너무 커지는 것을 방지하여 학습 안정성을 높입니다.

**3. 작동 방식:**

1.  **기울기 계산:** 모델의 손실 함수에 대한 각 매개변수(파라미터)의 기울기를 계산합니다.
2.  **분산 스무딩:** 과거 기울기 값을 지수적으로 감쇠시켜 분산 스무딩 계수를 사용하여 누적합니다.
3.  **학습률 업데이트:** 각 매개변수의 업데이트 크기를 다음과 같이 계산합니다.
    *   `새로운_매개변수_값 = 이전_매개변수_값 - 학습률 * (기울기 + 분산 스무딩된 이전 기울기)`
    *   여기서 ‘학습률’은 각 매개변수별로 조정되며, 분산 스무딩된 이전 기울기는 과거 기울기의 영향을 줄입니다.

**4. 응용 분야:**

*   **딥러닝 모델 학습:** 특히, ReLU 활성화 함수를 사용하는 심층 신경망(Deep Neural Networks)에서, 학습 속도를 높이고 수렴을 개선하는 데 효과적입니다.
*   **비선형 데이터:** 데이터가 복잡하고 비선형적인 경우, 학습률을 자동 조정하여 최적의 솔루션을 찾도록 돕습니다.
*   **전이 학습 (Transfer Learning):** 새로운 작업에 대해 학습하는 과정에서, 기존 모델의 가중치를 미세 조정할 때 사용됩니다.

**5. 관련 용어:**

*   **경사 하강법 (Gradient Descent):** 가장 기본적인 최적화 알고리즘으로, 손실 함수의 기울기를 따라 파라미터를 업데이트합니다.
*   **Adam (Adaptive Moment Estimation):** RMSprop의 변형으로, 모멘트 추정기를 사용하여 기울기의 평균과 분산을 동시에 추정합니다.
*   **Momentum:** 경사 하강법의 변형으로, 이전 업데이트 방향을 고려하여 학습 속도를 개선합니다.
*   **손실 함수 (Loss Function):** 모델의 예측값과 실제값의 차이를 측정하는 함수입니다.
*   **하이퍼파라미터 (Hyperparameter):** 모델 학습 전에 사람이 설정해야 하는 값입니다. (예: 학습률, 분산 스무딩 계수)
