# Attention (주의집중)

## Attention (주의집중)

**1. 정의:**

Attention은 인공신경망이 입력 데이터의 특정 부분에 더 집중하도록 하는 메커니즘입니다. 이는 모든 정보를 동일한 중요도로 취급하는 대신, 중요한 정보에 더 많은 가중치를 부여하여 모델의 성능을 향상시키는 기술입니다.

**2. 핵심 개념:**

*   **가중치 (Weights):** 입력 데이터의 각 부분에 부여되는 중요도를 나타내는 값입니다. Attention 메커니즘은 가중치를 통해 중요한 정보에 집중합니다.
*   **쿼리 (Query), 키 (Key), 값 (Value):** Attention 메커니즘에서 사용되는 세 가지 주요 구성 요소입니다. 쿼리는 원하는 정보의 특징을 나타내고, 키는 데이터의 특징을 나타내며, 값은 실제 데이터의 내용을 담고 있습니다.
*   **Self-Attention:** 동일한 입력 데이터 내에서 각 부분 간의 관계를 파악하는 Attention의 한 형태입니다.
*   **Multi-Head Attention:** 여러 개의 Attention 메커니즘을 병렬로 사용하여 다양한 관점에서 입력 데이터에 집중합니다.

**3. 작동 방식:**

1.  입력 데이터가 Attention 메커니즘에 입력됩니다.
2.  쿼리, 키, 값으로 분해됩니다.
3.  각 쿼리와 키 간의 유사도를 계산합니다. (Similarity Score)
4.  유사도 점수를 기반으로 각 값에 가중치를 부여합니다.
5.  가중치가 부여된 값들을 합쳐 최종 출력을 생성합니다.

**4. 응용 분야:**

*   **자연어 처리 (NLP):** 번역, 텍스트 요약, 질문 응답 등 다양한 NLP 작업에서 중요한 역할을 합니다.
*   **컴퓨터 비전 (Computer Vision):** 이미지 캡셔닝, 객체 탐지 등에서 이미지의 중요한 부분에 집중하여 모델의 정확도를 높입니다.
*   **음성 인식 (Speech Recognition):** 음성 데이터에서 중요한 단어나 구문에 집중합니다.
*   **음악 생성 (Music Generation):** 음악의 특정 부분에 집중하여 더욱 자연스러운 음악을 생성합니다.

**5. 관련 용어:**

*   **Transformer:** Attention 메커니즘을 기반으로 구축된 딥러닝 모델 아키텍처입니다.
*   **RNN (Recurrent Neural Network):** 시퀀스 데이터를 처리하는데 사용되는 딥러닝 모델입니다. Attention 메커니즘은 RNN의 단점을 보완합니다.
*   **Embeddings (임베딩):** 단어나 문장을 벡터 형태로 표현하는 방법입니다. Attention 메커니즘은 Embedding을 활용합니다.
*   **Positional Encoding:** 시퀀스 데이터의 위치 정보를 모델에 제공하는 방법입니다.
*   **Context Vector:** Attention 메커니즘의 출력으로 생성되는 벡터입니다.